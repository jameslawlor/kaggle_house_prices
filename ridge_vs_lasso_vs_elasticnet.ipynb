{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook produces a cross validation comparison of the linear models Ridge, Lasso and ElasticNet with $\\alpha$ tuning. At the time of publication this script performs in the top ~15% of submissions. Using a RMSE for evaluation it is found that ElasticNet appears slightly better than the Lasso and Ridge models. Some basic preprocessing and feature creation is also included but it should be emphasised that the median imputation used on missing values is very crude. For example, Area features with missing values may be this way because the property does not have that feature (e.g. a pool) so it would make more sense to set this to zero. Feature creation is done by taking the square root of all numerical area features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train = pd.read_csv(\"train.csv\",index_col=\"Id\")\n",
    "test = pd.read_csv(\"test.csv\",index_col=\"Id\")\n",
    "\n",
    "def print_full(x):\n",
    "    \"\"\"\n",
    "    Full printing of dataframes for error checking\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_columns', 999)\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "def clean(df):\n",
    "    \"\"\"\n",
    "    Cleans NaNs and creates new features\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of new features to be created: (new_feature, original_feature, transform_function)\n",
    "    transform = [(\"sqLotArea\",\"LotArea\",np.sqrt),\n",
    "                 (\"sqGrLivArea\",\"GrLivArea\",np.sqrt),\n",
    "                 (\"sqBsmtFinSF1\",\"BsmtFinSF1\",np.sqrt),\n",
    "                 (\"sqBsmtFinSF2\",\"BsmtFinSF2\",np.sqrt),\n",
    "                 (\"sqBsmtUnfSF\",\"BsmtUnfSF\",np.sqrt),\n",
    "                 (\"sqTotalBsmtSF\",\"TotalBsmtSF\",np.sqrt),\n",
    "                 (\"sq1stFlrSF\",\"1stFlrSF\",np.sqrt),\n",
    "                 (\"sq2ndFlrSF\",\"2ndFlrSF\",np.sqrt),\n",
    "                 (\"sqLotFrontage\",\"LotFrontage\",np.sqrt),\n",
    "                 (\"sqMasVnrArea\",\"MasVnrArea\",np.sqrt),\n",
    "                 (\"sqPoolArea\",\"PoolArea\",np.sqrt),\n",
    "                 (\"sqGarageArea\",\"GarageArea\",np.sqrt),\n",
    "                 (\"sqWoodDeckSF\",\"WoodDeckSF\",np.sqrt),\n",
    "                 (\"sqOpenPorchSF\",\"OpenPorchSF\",np.sqrt),\n",
    "                 (\"sqEnclosedPorch\",\"EnclosedPorch\",np.sqrt),\n",
    "                ]\n",
    "    \n",
    "    # Find categorical and numerical features\n",
    "    categoricals = train.select_dtypes(include=[\"object\"]).columns.values\n",
    "    numericals = [feat for feat in train.select_dtypes(include=[\"int\",\"float\"]).columns.values]\n",
    "    \n",
    "    # Remove NaNs... bear in mind this is a rough script. I recommend a more intelligent way of doing this,\n",
    "    # for example a feature like LotFrontage may be NaN because the property has no Lot Frontage, so it makes\n",
    "    # more sense to set this to zero instead of imputing the median value as shown below.\n",
    "    \n",
    "    # Transform to create new features, scale using MinMaxScaler\n",
    "    for (new_feature,original_feature,f) in transform: \n",
    "        df[new_feature] = df[original_feature].fillna(df[original_feature].median(), inplace = False)\n",
    "        df[new_feature] = MinMaxScaler().fit_transform(f(df[new_feature].apply(float)).reshape(-1,1))\n",
    "    # Scale and remove NaNs for numerical features by imputing median value\n",
    "    for feature in numericals: \n",
    "        df[feature].fillna(df[feature].median(), inplace = True)\n",
    "        df[feature] = MinMaxScaler().fit_transform(df[feature].apply(float).reshape(-1,1))\n",
    "    # Impute NaNs for categorical features\n",
    "    for feature in categoricals: \n",
    "        df[feature].fillna(df[feature].value_counts().idxmax(), inplace = True)\n",
    "    # Perform one hot encoding on the categorical features\n",
    "    for cat in categoricals:\n",
    "        dummies = pd.get_dummies(df[cat])\n",
    "        dummies.columns = [col_name + cat for col_name in dummies.columns.values]            \n",
    "        df = df.drop(cat,axis=1)\n",
    "        df = df.join(dummies)\n",
    "    return df\n",
    "\n",
    "target = train[\"SalePrice\"] # Note that we will take the Log of this when fitting - check the histogram of this feature\n",
    "train = train.drop(\"SalePrice\",axis=1)\n",
    "\n",
    "dd = clean(pd.concat([train,test]))\n",
    "\n",
    "train = dd[:len(train)]\n",
    "test = dd[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Lasso Regression RMSE=====\n",
      "0.0001 0.133060344119\n",
      "0.0003 0.127280305333\n",
      "0.001 0.129134911203\n",
      "0.003 0.146925504614\n",
      "=====Ridge Regression RMSE w/ alphas =====\n",
      "1 0.136784674567\n",
      "2 0.135535092275\n",
      "3 0.135105758434\n",
      "4 0.134995672138\n",
      "5 0.135054231221\n",
      "6 0.135215174773\n",
      "7 0.135443219182\n",
      "8 0.135717369914\n",
      "9 0.13602419713\n",
      "=====ElasticNet RMSE w/ alphas =====\n",
      "0.0003 0.129778843502\n",
      "0.0004 0.128521524153\n",
      "0.0005 0.127809961627\n",
      "0.0006 0.12731380798\n",
      "0.0007 0.126960267957\n",
      "0.0008 0.126761552613\n",
      "0.0009 0.126753655608\n",
      "0.001 0.126861278601\n",
      "0.0011 0.127057642503\n",
      "0.0012 0.127349421042\n",
      "0.0013 0.127710818952\n",
      "0.0014 0.128135595479\n",
      "0.0015 0.128616046301\n",
      "0.0016 0.129119230004\n",
      "0.0017 0.129623901034\n",
      "0.0018 0.130111067187\n",
      "0.0019 0.130612719001\n",
      "0.002 0.131124826853\n",
      "0.0021 0.131646049595\n",
      "0.0022 0.132179311413\n",
      "0.0023 0.132717100711\n",
      "0.0024 0.133206910762\n",
      "0.0025 0.133684209551\n",
      "0.0026 0.134160481608\n",
      "0.0027 0.134631426208\n",
      "0.0028 0.135097740364\n",
      "0.0029 0.135579483798\n",
      "=====LassoLars w/ alphas =====\n",
      "0.142594922935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.323e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/james/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.978e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/james/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.236e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/james/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.269e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/james/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.128e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/james/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LassoLarsCV\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "def rmse_cv(model): # Cross val using the competition scoring metric\n",
    "    return(np.sqrt(-cross_val_score(model, train, np.log(target), scoring=\"mean_squared_error\", cv = 5)))\n",
    "\n",
    "print(\"=====Lasso Regression RMSE=====\")\n",
    "for a in [1.0e-4,3.0e-4,1.0e-3,3.0e-3]:\n",
    "    print(a, rmse_cv(Lasso(alpha = a)).mean())\n",
    "    \n",
    "print(\"=====Ridge Regression RMSE w/ alphas =====\")\n",
    "for a in np.arange(1,10,1):\n",
    "    print(a, rmse_cv(Ridge(alpha = a)).mean())\n",
    "    \n",
    "print(\"=====ElasticNet RMSE w/ alphas =====\")\n",
    "#for a in [3.0e-4,1.0e-3,3.0e-3,1.0e-4]:\n",
    "for a in np.arange(3.0e-4,3.0e-3,1.0e-4):\n",
    "    print(a, rmse_cv(ElasticNet(alpha = a,max_iter=10000)).mean())\n",
    "\n",
    "    \n",
    "print(\"=====LassoLars w/ alphas =====\")\n",
    "print(rmse_cv(LassoLars(alpha=0.000496269234175)).mean())\n",
    "    \n",
    "# Best RMSE is ElasticNet with alpha = 0.0009\n",
    "best_model = ElasticNet(alpha=0.0009).fit(train,np.log(target))\n",
    "\n",
    "# Output to CSV\n",
    "test[\"SalePrice\"] = np.exp(best_model.predict(test))\n",
    "test[[\"SalePrice\"]].to_csv(\"submit.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# train = pd.read_csv(\"train.csv\",index_col=\"Id\")\n",
    "# print(train[\"LotFrontage\"].isnull().sum())\n",
    "# print(len(train[\"LotFrontage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
